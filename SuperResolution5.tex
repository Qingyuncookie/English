\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{subfig}

\graphicspath{{/home/li/图片/}}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{fontspec}

\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}

\begin{document}

\author{Qingyun Li\\\\
June 15, 2018}        
\title{Super Resolution}

\maketitle

\begin{abstract}
 \par
  Image resolution is an important indicator of image detail rendering capability. It describes the number of pixels contained in an image. However, due to the limitations of the hardware conditions of the image acquisition system, we can't acquire the high-resolution image. Image super resolution refers to the recovery of high-resolution image from low-resolution image or image sequence. There are three methods in general, interpolation, reconstrution and learning.  High resolution means that the pixels in the image are high in density and can provide more detail, and these are very helpful. For example, the doctors can make a correct diagnosis according to the high resolution medical resolutions. And we can distinguish similar objects from the high-resolution satellite images. Therefore, the performance of pattern recognition will be greatly improved. So it is critical to research the high-resolution images. This paper will focus on the method based on learning.
\end{abstract}
\section{Artificial neural net}
 \par Artificial neural net models have been studied for many years in the hope of achiving human-like performance in the fields of speech and image recognition. These models are composed of many nonlinear computational elements operating in parallel and arranged in patterns reminiscent of biological neural nets. Computational elements or nodes are connected via weights that are typically adapted during use to improve performance. There has been a recent resurgence in the field of artificial nerual nets caused by new net topologies and algorithms, analog VLSI implementation techniques, and the belief that massive parallelism is essential for high performance speech and image recognitio~\cite{lippmann1987introduction}.
\subsection{Naural net and traditional classifiers}
\par Block diagrams of traditional and neural net classifiers are presented in Fig.~\ref{block} Both types of classifiers determine which of M classes is most representative of an unknown static input pattern containing N input elements. In a speech, recognizer the inputs might be the output envelope values from a filter bank spectral analyzer sampled at one time instant and the classes might represent different vowels. In an image classifier the inputs might be the gray scale level of each pixel for a picture and the classes might represent different objects.
\par The top of the figure is the traditional classifier and the bottom of the figure is an adaptive nerual net classifier. The troditional classifier contains two stages. The first computes matching scores for each class and the second selects the class with the maximum score. And multivariate gaussian distributions are often used leading to relatively simple algorithms for computing matching scores~\cite{elman1988learning}.
 \begin{figure}[htbp]
 \centering{}
\includegraphics[width=0.7\linewidth]{BlockDiagrams.png}\\
 \caption{Block diagrams}
\label{block}
\end{figure}
\par An adaptive neural net classifier is shown at the bottom of Fig.~\ref{block}. Here input values are fed in parallel to the first stage N input connections. Each connection carries an analog value which may take on two levels for binary inputs. The first stage computes matching scores and outputs these scores in parallel to the next stage over M analog output lines. Here the maximum of these values is selected and enhanced. The second stage has one output for each of the M classes.
\subsection{The Hamming net}
\par First, we explain what is hamming distance. At Wikipadia, the hamming distance between two strings of eaqul length is the number of positions at which the corresponding symbols are different in information theory. In other words, it measures the minimum number of substitutions required to change one string into the other, or the minimum number of errors that could have transformed one string into the other.
\par The Hopfield net is often tested on problems where inputs are generated by selecting an exemplar and reversing bit values randomly and independently with a given probability~\cite{hopfield1982neural}. This is a classic problem in communications theory that occurs when binary symmetric channel. The optimum minimum error classifier in this case calculates the Hamming distance to the exemplar for each class and selects that class with the minimum Hamming distance. The Hamming distance is the number of bits in the input which do not match the corresponding exemplar bits. A net which will be called a Hamming net implements this algorithm using naural net components and is shown in Fig.~\ref{Hamming}.
 \begin{figure}[htbp]
 \centering{}
\includegraphics[width=0.7\linewidth]{Hamming.png}\\
 \caption{Hamming net}
\label{Hamming}
\end{figure}
\subsection{Selecting or enhance the maximum input}
\par The need to select or enhance the input with a maximum value occurs frequently in classification problems. Several different neural nets can perform this operation. The MAXNET described above uses heavy lateral inhibition similar to that used in other net designs where a maximum was desired~\cite{hopfield1986computing}. These designs creat a "winner-take-all" type of net whose design mimics the heavy use of lateral inhibition evident in the biological neural nets of the hunman brain~\cite{kandel2000principles}. Other techniques to pick a maximum are also possible~\cite{lippmann1987comparison}. One is illustrated in Fig.~\ref{comparator}. This figure shows a comparator subnet which is described in~\cite{martin1970acoustic}. It uses threshold logic nodes to pick the maximum of two inputs and then feeds this maximum value forword. This net is useful when the maximum value must be passed unaltered to the output. 
\begin{figure}[htbp]
 \centering{}
\includegraphics[width=0.7\linewidth]{comparator.png}\\
 \caption{A comparator subnet that selects the maximum of two analog inputs.}
\label{comparator}
\end{figure}
\subsection{The Carpenter/Grossberg classifier}
\par Carpenter and Grossberg~\cite{carpenter1987neural}, in the development of their Adaptive Resonance Theory have designed a net which forms clusters and trained without supervision. This net implements a clustering algorithm described in~\cite{Hartigan1975Clustering}. This leader algorithm selects the first input as the examplar for the first cluster. It follows the leader and is clustered with the first if the distance to the first is less than a threshold. Otherwise it is the exemplar for a new cluster. This process is repeated for all following inputs. The number of clusters thus grows with time and depends on both the threshold and the distance metric used to compare inputs to cluster exemplars.
\par The major components of a Carpenter/Grossberg classification net with three inputs and two output nodes is presented in Fig.~\ref{carp}.
\begin{figure}[htbp]
 \centering{}
\includegraphics[width=0.7\linewidth]{carpenter.png}\\
 \caption{The major component.}
\label{carp}
\end{figure}
\subsection{Single layer perceptron}
\par The single layer perceptron is the first of three nets from the taxonomy in Fig.~\ref{single1} that can be used with both continuous valued and binary inputs. This simple net generated much interest when initially developed because of its ability to learn to recognize simple patterns. A perceptron that decides whether an input belongs to one of two classes (denoted A or B) is shown in the top of Fig.~\ref{single2}. The single node computes a weighted sum of the input elements, substracts a threshold ($\theta$) and passes the result through a hard limiting nonlinearity such that the output y is either +1 or -1. The decision rule is to respond class A if the output is +1 and class B if the output is -1. A useful technique for analyzing the behaviour of nets such as the perceptron is to plot a map of the decision regions created in the multidimensional space spanned by the input variables.
\begin{figure}[htbp]
 \centering{}
\includegraphics[width=0.7\linewidth]{singlelayer1.png}\\
 \caption{A taxonomy of six neural nets.}
\label{single1}
\end{figure}
\begin{figure}[htbp]
 \centering{}
\includegraphics[width=0.7\linewidth]{singlelayer2.png}\\
 \caption{A single layer perceptron.}
\label{single2}
\end{figure}
 \bibliographystyle{ieee}
 \bibliography{single}
\end{document}
